---
title: "Final_data_clean"
author: "Group2"
date: "2024-08-21"
output:
  html_document:
    code_folding: hide
    toc: true
    toc_depth: 3
    toc_float:
      collapsed: true
      smooth_scroll: true
table-of-contents: true 
number-sections: true 
embed-resources: true 
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
library(tidyverse)
library(here)      
library(readxl)    
library(janitor)   
library(stringr)   
library(tidyr)     
library(dplyr)
library(knitr)
library(ggplot2)
library(factoextra)
library(gt)
library(naniar)
library(ggcorrplot)
library(corrplot)
library(GGally)
library(randomForest)
library(pdp)
library(e1071)  
library(dplyr) 
library(caret)
library(plotly)
library(stargazer)
library(broom)
library(knitr)
library(kableExtra)
```


# Data cleaning & EDA & Pre-processing

## Section 1: Load in the datasets
We first load in the useful data sets we need including the original population and indigenous population. Here we first merged two datasets (bb/bp) for original populations using primary key (ABSPID). We also merged three data sets (bsp/bhh/bcn) for indigenous population by (ABSGID & ABSHID). We renamed the column names to keep consistency.

At the same time, we then add a column which indicate whether the person is in the original population or indigenous populaiotn (binary result).
```{r}
raw_bp <- read.csv(here("ZIPallFiles","AHSnpa11bp.csv"), header=TRUE)
raw_bb <- read.csv(here("ZIPallFiles","AHSnpa11bb.csv"), header=TRUE)
bp_bb <- inner_join(raw_bp, raw_bb, by = "ABSPID") %>%
  rename(SOCIAL = SF2SA1QN) %>%
   mutate(Identity = factor(0))

raw_bsp <- read.csv(here("ZIPallFiles","inp13bsp.csv"), header=TRUE)
raw_bcn <- read.csv(here("ZIPallFiles","inp13bcn.csv"), header=TRUE)
raw_bhh <- read.csv(here("ZIPallFiles","inp13bhh.csv"), header=TRUE)



bsp_bcn_bhh<- left_join(raw_bsp, raw_bhh,by = c("ABSHID")) %>%
  filter(!ABSHID %in% raw_bcn$ABSHID[raw_bcn$ICD10ME %in% c(7, 20)])%>%
  rename(AGEC =AGEEC, SOCIAL = SF2SA1DB)%>%
   mutate(Identity = factor(1))
```
When loading the indigenous population data set, we also filter out the people who have hypertension and taking medication.

## Section 2: Select and merge
We start to clean the data and first as we only looking at the population who are not currently have medication relating to blood pressure, also we focus on the adults and population who are not currently pregnant, so we only focus on the target population in our analysis.

```{r}
bp_bb <- bp_bb %>%
  filter(
    AGEC >= 18, 
    HYPBC == 5,
    SABDYMS != 4
    )
bsp_bcn_bhh <- bsp_bcn_bhh %>% 
  filter(
    AGEC >= 18, 
    SABDYMS != 4)
```


Next, we merged two populations into the one whole population and name this big data set as raw_data.

```{r}
raw_data <- bind_rows(bp_bb,bsp_bcn_bhh)
```

The useful variables then been selected including all predictors, response variables, and confounding variables.

```{r}
selected_data <- raw_data %>%
select(
    ABSPID, ABSHID, SOCIAL, AGEC, SEX, BMISC, SYSTOL, DIASTOL,
    SMKSTAT, ALCPER1, ALCPER2,
    POTAST1, POTAST2, 
    SODIUMT1, SODIUMT2,
    FIBRPER1, FIBRPER2, 
    CHOPER1, CHOPER2,
    Identity
  )


```

## Section 3: Missing values encoding & data types conversion & Imbalance class

### 3.1 Missing values encoding and data types conversion
In next step we encode all missing values base on the data dictionary and correct all data types. At the same time we calculate the average of those two-days variables.

```{r}

variable_info <- data.frame(
  Variable = names(selected_data),
  Type = sapply(selected_data, class)
)

variable_info %>%
  gt() %>%
  tab_header(
    title = "Variable Names and Their Types"
  ) %>%
 tab_caption(caption = md("Table 1: Variable types table"))
```


```{r}
selected_data <- selected_data %>%
  mutate(
    across(c(SEX, SOCIAL, SMKSTAT), ~ as.factor(.)),
    BMISC = na_if(na_if(BMISC, 99), 98),
    SYSTOL = na_if(na_if(na_if(SYSTOL,0),998),999),
    DIASTOL = na_if(na_if(na_if(DIASTOL,0),998),999),
    POTAST = (POTAST1 + POTAST2) / 2, 
    SODIUMT = (SODIUMT1 + SODIUMT2) / 2,
    FIBRPER = (FIBRPER1 + FIBRPER2)/2,
    CHOPER = (CHOPER1 + CHOPER2) / 2,
    ALCPER = (ALCPER1 + ALCPER2) /2
    ) %>%
 select(
    ABSPID,ABSHID, SOCIAL, AGEC, SEX, BMISC, SYSTOL, DIASTOL,
    SMKSTAT, ALCPER,
    POTAST, 
    SODIUMT,
    FIBRPER, 
    CHOPER,
    Identity
  )
```

We had a look at the types after we changed.

```{r}

variable_info <- data.frame(
  Variable = names(selected_data),
  Type = sapply(selected_data, class)
)

variable_info %>%
  gt() %>%
  tab_header(
    title = "Variable Names and Their Types"
  ) %>%
 tab_caption(caption = md("Table 2: Variable types table after correction"))
```

### 3.2 Factor variables class distribution
```{r}

plot_categorical_distribution <- function(data) {
  categorical_vars <- data %>%
    select(where(is.factor) | where(is.character)) %>%
    select(-ABSPID, -ABSHID)
  
  for (var in names(categorical_vars)) {
    non_na_data <- data %>%
      filter(!is.na(.data[[var]]))
    
    if (nrow(non_na_data) == 0) next
    
    p <- ggplot(non_na_data, aes(x = .data[[var]])) +
      geom_bar(aes(y = (..count..) / sum(..count..), fill = .data[[var]]), color = "black") +
      scale_fill_brewer(palette = "Set3") + 
      labs(title = paste("Proportion of Categories in", var), 
           x = "Category", 
           y = "Proportion") +
      theme_minimal() +
      theme(axis.text.x = element_text(angle = 45, hjust = 1))  
    
    print(p)
  }
}

plot_categorical_distribution(selected_data)       
```

Base on the dictionary there are no big difference between category 1, 2, and 3, as they are all people who currently smoke.

```{r}
#selected_data <- selected_data %>% 
 # mutate(SMKSTAT = recode(SMKSTAT, "2" = "1", "3" = "1","4" = "2","5" = "3"))

```

```{r}
plot_categorical_distribution(selected_data) 
```


## Section 4: Duplicate rows identification

Then the duplicate values been checked, we found no duplicate values.
```{r}

unique_data <- selected_data %>%
  select(-ABSPID) %>%
  distinct()

dims <- dim(unique_data)

dims_table <- tibble::tibble(
  Dimension = c("Rows", "Columns"),
  Count = dims
)
dims_table %>%
  gt() %>%
  tab_header(
    title = "Dimensions of Unique Tibble"
  )%>%
 tab_caption(caption = md("Table 3: Dimensions table"))
```



## Section 5: Missing values cleaning

### 5.1 Missing values identification
The summary statistic of missing values been shown below, we found there occurs some missingness in BMI and blood pressure measurments.

```{r warning = FALSE}
data2 <- selected_data %>%
  select(-ABSPID,-ABSHID)
miss_summary <- miss_var_summary(data2)
kable(miss_summary, digits = 2, format = "html")
```

Then looking in to the missingness pattern,this missingness plot provides an overview of missing values for each variable in our data set, the majority of variables have 0% missing values, with only three variables showing missing data (BMISC, SYSTOL, and DIASTOL) at around 13-14%.
The missing values seem to follow a consistent pattern across the observations (rows) for the BMISC, SYSTOL, and DIASTOL variables. 

```{r}
vis_miss(data2)
```

### 5.2 Regression imputation for missing values

Base on the patterns we found, we decide to use regression imputation to deal with those small proportion of missingness,

```{r}
data <- selected_data 
corr_matrix <- data%>%
  select(where(is.numeric)) %>%
  mutate(across(everything(), ~ ifelse(is.na(.), mean(., na.rm = TRUE), .))) %>%
  cor(use = "complete.obs")

ggcorrplot(corr_matrix, lab = TRUE, 
           title = "Correlation Heatmap for ALL", 
           colors = c("blue", "white", "red"),
           tl.cex = 9, lab_size = 4) + theme(legend.position = "none")

```
AS observed from the heat map, we can tell that the two most significant (except blood pressure) multicollinearity is between energy percentage from carbohydrate and fibers, also between the sodium and potassium. So we decide to only choose one between each to do the regression imputation for missing values.


```{r}
# 1: Impute missing values for BIMISC
model_BMISC <- lm(BMISC ~ SEX + SOCIAL + AGEC + POTAST + FIBRPER + ALCPER + SMKSTAT + Identity, data = data, na.action = na.exclude)

data$BMISC[is.na(data$BMISC)] <- predict(model_BMISC, newdata = data[is.na(data$BMISC), ])

# 2: Impute missing values for SYSTOL
model_SYSTOL <- lm(SYSTOL ~ SEX + SOCIAL + AGEC + BMISC + POTAST + FIBRPER + ALCPER + SMKSTAT + Identity, data = data, na.action = na.exclude)

data$SYSTOL[is.na(data$SYSTOL)] <- predict(model_SYSTOL, newdata = data[is.na(data$SYSTOL), ])

# 3: Impute missing values for DIASTOL
model_DIASTOL <- lm(DIASTOL ~ SEX + SOCIAL + AGEC + BMISC + SYSTOL + POTAST  + FIBRPER + ALCPER + SMKSTAT + Identity, data = data, na.action = na.exclude)
data$DIASTOL[is.na(data$DIASTOL)] <- predict(model_DIASTOL, newdata = data[is.na(data$DIASTOL), ])

# Check the imputed values
data1 <- data %>%
  select(-ABSPID,-ABSHID)
vis_miss(data1)
selected_data <- data 
```







## Section 6: Outlier detection

Next we checked the outliers in each numeric variables, we had observed that all the variables except the **AGEC** (which indicate the age of the person) contain ouliers,
```{r,fig.height=10}
numerical_vars <- selected_data %>%
  select_if(is.numeric) 


par(mfrow = c(3, 3)) 
for (var in names(numerical_vars)) {
  boxplot(numerical_vars[[var]], 
          main = var, 
          ylab = var, 
          col = "lightblue",        
          border = "darkblue",      
          outcol = "red",           
          pch = 19                
  )
}
```

As some of those outliers are stil plausible values, we still want to keep them, but we need to consider those inconsistence values which is not in the correct range.


```{r}
normal_ranges <- list( SYSTOL = c(90, 180),  
                       DIASTOL = c(60, 120), 
                       BMISC = c(16, 47.5),
                       PS_Ratio=c(0,4),
                       ALCPER = c(0, 90),
                       CHOPER = c(0, 100),
                       FIBRPER = c(0, 100)
                       ) 
                       
filtered_data <- selected_data %>%
  mutate(
    PS_Ratio = SODIUMT/POTAST
  )%>%
  filter(
    SYSTOL >= normal_ranges$SYSTOL[1] & SYSTOL <= normal_ranges$SYSTOL[2],
    DIASTOL >= normal_ranges$DIASTOL[1] & DIASTOL <= normal_ranges$DIASTOL[2],
    BMISC >= normal_ranges$BMISC[1] & BMISC <= normal_ranges$BMISC[2],
    ALCPER>= normal_ranges$ALCPER[1] & ALCPER <= normal_ranges$ALCPER[2],
    CHOPER >= normal_ranges$CHOPER[1] & CHOPER <= normal_ranges$CHOPER[2],
    FIBRPER >= normal_ranges$FIBRPER[1] & FIBRPER <= normal_ranges$FIBRPER[2],
    PS_Ratio >= normal_ranges$PS_Ratio[1] & PS_Ratio <= normal_ranges$PS_Ratio[2]
  )
filtered_data 
```





## Section 7: Calculating extra response & confounding variables 

Subsequently, we add two columns base on the values of two blood pressure measurements. Also calculate the ratio of the sodium and potassium for one of the confounding variable.

```{r}
filtered_data <- filtered_data %>%
  mutate(
    Hypertension = as.factor(case_when(
    SYSTOL >= 120 | DIASTOL >= 80 ~ 1,
    TRUE ~ 0)),
    
    BP_Category = as.factor(case_when(
    SYSTOL < 120 & DIASTOL < 80 ~ 0,      
    SYSTOL >= 120 & SYSTOL <= 129 & DIASTOL < 80 ~ 1,  
    TRUE ~ 2   )),
    
    PS_Ratio = SODIUMT/POTAST
  )%>%
  select(-SODIUMT,-POTAST)

```

# Modelling (without CV for tuning)
## Section 0 Normalisation

```{r}
numeric_cols <- sapply(filtered_data, is.numeric)  

normalized_data <- filtered_data
normalized_data[numeric_cols] <- scale(filtered_data[numeric_cols])
```


## Section 1 Logistic model 

### 1.1 multicollinearity
```{r}
a <- filtered_data %>%
  select(where(is.numeric),-SYSTOL,-DIASTOL)
ggscatmat(a)
```

### 1.2 Logistic regression 
```{r}

r <- glm(Hypertension ~ SOCIAL + SEX + AGEC + BMISC + SMKSTAT + ALCPER + FIBRPER + CHOPER + Identity + PS_Ratio, 
         data = normalized_data, family = binomial())

summary(r)
```



## Section 2 Random Forest




```{r}
# Set a seed for reproducibility
set.seed(123)


rf_model <- randomForest(Hypertension ~ SOCIAL + SEX + AGEC + BMISC + SMKSTAT + ALCPER + FIBRPER + CHOPER + Identity + PS_Ratio,
                         data = normalized_data, 
                         ntree = 500)  

rf_model
```




## Section 3 SVM
```{r}

dummy_model <- dummyVars(~ . - Hypertension - ABSPID - ABSHID - BP_Category - SYSTOL - DIASTOL, data = normalized_data)

encoded_data <- as.data.frame(predict(dummy_model, newdata = normalized_data))

final_data <- as.data.frame(cbind(encoded_data, Hypertension = normalized_data$Hypertension))

set.seed(123)

svm_model <- svm(Hypertension ~ ., 
                 data = final_data,  
                 kernel = "radial",  
                 cost = 1,  
                 gamma = 0.1)  


svm_model
```





# Model comparison (5-fold cross validation)

Then we done the cross validation for three models, as for the validity we perform all cross validation for three models base on the same 5 folds 

Measurements decision:

1. Accuracy: As the response variable(hypertension) is balanced in our dataset, we consider the accuracy is plausible in the comparison

2. Sensitivity: As the respons variable is the risk of the disease, we wanna reduce the case that the person is actually have risk of hypertension but been predict as not have any risk, so in this case we want to have a smallest Type II error(false negative), sensitivity would be the best measurement in this case.






```{r}
set.seed(1234)
k_folds <- 5
folds <- createFolds(normalized_data$Hypertension, k = k_folds, list = TRUE, returnTrain = FALSE)

# Initialize variables to store performance results
logistic_accuracy <- numeric(k_folds)
logistic_f1 <- numeric(k_folds)
logistic_sensitivity <- numeric(k_folds)

rf_accuracy <- numeric(k_folds)
rf_f1 <- numeric(k_folds)
rf_sensitivity <- numeric(k_folds)

svm_accuracy <- numeric(k_folds)
svm_f1 <- numeric(k_folds)
svm_sensitivity <- numeric(k_folds)

# Helper function to ensure factor levels match
ensure_factor_levels <- function(predictions, reference) {
  # Ensure predictions have the same levels as the reference
  factor(predictions, levels = levels(reference))
}


for (i in 1:k_folds) {
  
  # Split into training and testing sets
  test_indices <- folds[[i]]
  test_data <- normalized_data[test_indices, ]
  train_data <- normalized_data[-test_indices, ]
  
  ### Logistic Regression ###
  log_model <- glm(Hypertension ~ SOCIAL + SEX + AGEC + BMISC + SMKSTAT + ALCPER + FIBRPER + CHOPER + Identity + PS_Ratio, 
                   data = train_data, family = binomial())
  log_predictions <- predict(log_model, test_data, type = "response")
  log_class <- ifelse(log_predictions > 0.5, 1, 0)
  log_class <- ensure_factor_levels(log_class, test_data$Hypertension)
  
  # Compute accuracy, F1 score, and sensitivity for logistic regression
  log_conf_matrix <- confusionMatrix(log_class, test_data$Hypertension, positive = "1")
  
  # Store accuracy, F1 score, and sensitivity
  logistic_accuracy[i] <- log_conf_matrix$overall['Accuracy']
  logistic_f1[i] <- log_conf_matrix$byClass['F1']
  logistic_sensitivity[i] <- log_conf_matrix$byClass['Sensitivity']
  
  
   ### Random Forest ###
  rf_model <- randomForest(Hypertension ~ SOCIAL + SEX + AGEC + BMISC + SMKSTAT + ALCPER + FIBRPER + CHOPER + Identity + PS_Ratio,
                           data = train_data, 
                           ntree = 500)
  
  # Get predictions for random forest
  rf_predictions <- predict(rf_model, test_data)
  
  # Ensure factor levels match before creating confusion matrix
  rf_predictions <- ensure_factor_levels(rf_predictions, test_data$Hypertension)
  
  # Compute accuracy, F1 score, and sensitivity for Random Forest
  rf_conf_matrix <- confusionMatrix(rf_predictions, test_data$Hypertension, positive = "1")
  
  # Store accuracy, F1 score, and sensitivity
  rf_accuracy[i] <- rf_conf_matrix$overall['Accuracy']
  rf_f1[i] <- rf_conf_matrix$byClass['F1']
  rf_sensitivity[i] <- rf_conf_matrix$byClass['Sensitivity']
  
  ### SVM ###
  # One-hot encode categorical variables for SVM model
  dummy_model <- dummyVars(~ . - Hypertension - ABSPID - ABSHID - BP_Category - SYSTOL - DIASTOL, data = train_data)
  encoded_train_data <- as.data.frame(predict(dummy_model, newdata = train_data))
  encoded_test_data <- as.data.frame(predict(dummy_model, newdata = test_data))
  encoded_train_data$Hypertension <- train_data$Hypertension
  encoded_test_data$Hypertension <- test_data$Hypertension
  
  # Fit the SVM model with one-hot encoded data
  svm_model <- svm(Hypertension ~ ., 
                   data = encoded_train_data, 
                   kernel = "radial",  
                   cost = 1,  
                   gamma = 0.1)
  
  # Get predictions for SVM
  svm_predictions <- predict(svm_model, encoded_test_data)
  
  # Ensure factor levels match before creating confusion matrix
  svm_predictions <- ensure_factor_levels(svm_predictions, encoded_test_data$Hypertension)
  
  # Compute accuracy, F1 score, and sensitivity for SVM
  svm_conf_matrix <- confusionMatrix(svm_predictions, encoded_test_data$Hypertension, positive = "1")
  
  # Store accuracy, F1 score, and sensitivity
  svm_accuracy[i] <- svm_conf_matrix$overall['Accuracy']
  svm_f1[i] <- svm_conf_matrix$byClass['F1']
  svm_sensitivity[i] <- svm_conf_matrix$byClass['Sensitivity']
}

# Calculate the average accuracy, F1 score, and sensitivity across the 5 folds for logistic regression
avg_logistic_accuracy <- mean(logistic_accuracy)
avg_logistic_f1 <- mean(logistic_f1)
avg_logistic_sensitivity <- mean(logistic_sensitivity)

# Calculate the average accuracy, F1 score, and sensitivity across the 5 folds for random forest
avg_rf_accuracy <- mean(rf_accuracy)
avg_rf_f1 <- mean(rf_f1)
avg_rf_sensitivity <- mean(rf_sensitivity)

# Calculate the average accuracy, F1 score, and sensitivity across the 5 folds for SVM
avg_svm_accuracy <- mean(svm_accuracy)
avg_svm_f1 <- mean(svm_f1)
avg_svm_sensitivity <- mean(svm_sensitivity)

# Print the results
cat("Logistic Regression - Average Accuracy:", avg_logistic_accuracy, " Average Sensitivity:", avg_logistic_sensitivity, "\n")
cat("Random Forest - Average Accuracy:", avg_rf_accuracy,  " Average Sensitivity:", avg_rf_sensitivity, "\n")
cat("SVM - Average Accuracy:", avg_svm_accuracy, " Average Sensitivity:", avg_svm_sensitivity, "\n")

```


```{r}


# Data for plotting
models <- c("Logistic Regression", "Random Forest", "SVM")
avg_accuracies <- c(avg_logistic_accuracy, avg_rf_accuracy, avg_svm_accuracy)  # Replace with actual values
avg_sensitivities <- c(avg_logistic_sensitivity, avg_rf_sensitivity, avg_svm_sensitivity)  # Replace with actual values

# Create plotly figure
fig <- plot_ly()

# Add line for accuracy
fig <- fig %>% add_trace(x = models, y = avg_accuracies, type = 'scatter', mode = 'lines+markers',
                         name = 'Accuracy', line = list(color = 'lightblue', width = 3),
                         marker = list(size = 10))

# Add line for sensitivity
fig <- fig %>% add_trace(x = models, y = avg_sensitivities, type = 'scatter', mode = 'lines+markers',
                         name = 'Sensitivity', line = list(color = 'salmon', width = 3),
                         marker = list(size = 10))

# Customize layout, including y-axis range
fig <- fig %>% layout(title = "Model Comparison: Accuracy vs Sensitivity",
                      xaxis = list(title = "Model"),
                      yaxis = list(title = "Score", range = c(0.6, 0.8)),  # Adjust y-axis range
                      legend = list(title = list(text = "Metrics")),
                      font = list(family = "Arial", size = 14, color = "RebeccaPurple"))

# Display the plot
fig


```

Final model choose: logistic regression

# Interpretation on the final model

```{r}

# Train the logistic regression model
final_log_model <- glm(Hypertension ~ SOCIAL + SEX + AGEC + BMISC + SMKSTAT + ALCPER + FIBRPER + CHOPER + Identity + PS_Ratio, 
                       data = normalized_data, family = binomial())

# Tidy up the model results using broom
tidy_log_model <- tidy(final_log_model, conf.int = TRUE)  # Add confidence intervals

# Display the table in a well-formatted way using kableExtra
tidy_log_model %>%
  kable(format = "html", digits = 3, 
        col.names = c("Term", "Estimate", "Std. Error", "z value", "P-value", "Conf. Low", "Conf. High")) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = FALSE) %>%
  save_kable("logistic_regression_output.html")
```

```{r message= FALSE}

odds_ratios <- exp(coef(final_log_model))
conf_intervals <- exp(confint(final_log_model))

# Create a data frame for plotting
coef_data <- data.frame(
  Predictor = names(odds_ratios),
  OddsRatio = odds_ratios,
  LowerCI = conf_intervals[, 1],
  UpperCI = conf_intervals[, 2]
)

# Plot the odds ratios
ggplot(coef_data, aes(x = Predictor, y = OddsRatio)) +
  geom_point(size = 3) +
  geom_errorbar(aes(ymin = LowerCI, ymax = UpperCI), width = 0.2) +
  geom_hline(yintercept = 1, linetype = "dashed", color = "red") +
  coord_flip() +  # Flip coordinates for a better view
  labs(title = "Odds Ratios with 95% Confidence Intervals",
       y = "Odds Ratio", x = "Predictor")
```

```{r}
install.packages("stargazer")

```


















